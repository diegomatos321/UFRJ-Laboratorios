{"cells":[{"cell_type":"markdown","id":"da1831fb","metadata":{"id":"da1831fb"},"source":["# Lista 4\n","### NCG-040 - Introdução à Visão Computacional\n","\n","**Instruções:**\n","\n"," 1. Comentários nos códigos desenvolvidos.\n"," 1. Caso exista uma pergunta, deve ser respondida como comentários no arquivo ou em uma nova célula de texto.\n"," 1. Digitar seu nome e DRE abaixo.\n"," 1. Ao finalizarem a lista, renomeie ou salve uma cópia do arquivo no formato NomeSobrenome_ListaX.ipynb\n"," 1. Enviar via Dropbox até 25/06/2025 - https://www.dropbox.com/request/Duz3Wq2nsdIYQVp7PVvH"]},{"cell_type":"markdown","source":["`Nome: `\n","\n","`DRE: `"],"metadata":{"id":"fd4qxghLFIqh"},"id":"fd4qxghLFIqh"},{"cell_type":"markdown","id":"208e1ea2","metadata":{"id":"208e1ea2"},"source":["## Opção 1: YOLO (You Only Look Once)\n","\n","1. **Configuração do Ambiente**\n","   - Instale as dependências necessárias (e.g., `torch`, `opencv-python`, `yolov5`).\n","   - Importe as bibliotecas.\n","\n","2. **Inferência com Modelo Pré-Treinado**\n","   - Carregue o modelo YOLO de sua preferência.\n","   - Aplique a detecção em imagens de demonstração.\n","   - Exiba resultados com bounding boxes.\n","\n","3. **Treinamento em Dataset Personalizado**\n","   - Prepare um pequeno dataset de imagens anotadas (`images/` e `labels/`).\n","   - Ajuste o arquivo de configuração para apontar para seu dataset.\n","   - Realize treinamento rápido (apenas algumas épocas) e monitore métricas.\n","\n","4. **Avaliação e Métricas**\n","   - Calcule Precision, Recall e mAP para seu modelo.\n","   - Plote gráficos de Loss e mAP durante o treinamento.\n","\n","5. **Melhorias e Experimentação**\n","   - Ajuste hiperparâmetros (tamanho de batch, learning rate) e compare resultados.\n","   - Teste diferentes tamanhos de input (320x320 vs 640x640).\n"]},{"cell_type":"code","execution_count":null,"id":"348dfc9a","metadata":{"id":"348dfc9a"},"outputs":[],"source":["# Parte 1: YOLO\n","\n","# 1. Instalação de dependências (execute apenas no Colab)\n","!pip install torch torchvision opencv-python yolov5\n","\n","# 2. Importações\n","import torch\n","import cv2\n","from yolov5 import YOLOv5  # ou use import ultralytics\n","\n","# 3. Carregar modelo pré-treinado\n","model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n","\n","# 4. Inferência em imagem de exemplo\n","img_path = 'caminho/para/sua/imagem.jpg'\n","img = cv2.imread(img_path)\n","results = model(img)\n","\n","# 5. Mostrar resultados\n","results.render()  # desenha bounding boxes\n","from matplotlib import pyplot as plt\n","plt.imshow(cv2.cvtColor(results.imgs[0], cv2.COLOR_BGR2RGB))\n","plt.axis('off')\n"]},{"cell_type":"markdown","id":"fc4ab194","metadata":{"id":"fc4ab194"},"source":["## Opção 2: Vision Transformers (ViT)\n","\n","1. **Introdução ao ViT**\n","   - Entenda a arquitetura de patch embedding e self-attention.\n","\n","2. **Implementação de Patch Embedding**\n","   - Escreva sua própria classe `PatchEmbedding` em PyTorch.\n","\n","3. **Uso de Modelo Pré-Treinado**\n","   - Utilize um ViT pré-treinado (`timm`, `torchvision.models`) para classificação.\n","   - Carregue e avalie em um conjunto de validação leve (e.g., CIFAR-10).\n","\n","4. **Visualização de Mapas de Atenção**\n","   - Extraia os pesos de atenção de uma camada específica.\n","   - Plote um mapa de atenção sobre uma imagem de entrada.\n","\n","5. **Fine-Tuning**\n","   - Congele as camadas iniciais e treine apenas o cabeçalho de classificação num pequeno dataset.\n","   - Compare acurácia antes e depois do fine-tuning.\n"]},{"cell_type":"code","execution_count":null,"id":"acda63a4","metadata":{"id":"acda63a4"},"outputs":[],"source":["# Parte 2: Vision Transformers\n","\n","# 1. Instalação de dependências\n","!pip install torch torchvision timm\n","\n","# 2. Importações\n","import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","from torchvision.datasets import CIFAR10\n","from torch.utils.data import DataLoader\n","import timm\n","\n","# 3. Implementação de PatchEmbedding\n","class PatchEmbedding(nn.Module):\n","    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=64):\n","        super().__init__()\n","        self.patch_size = patch_size\n","        self.projection = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n","    def forward(self, x):\n","        x = self.projection(x)\n","        x = x.flatten(2).transpose(1, 2)\n","        return x\n","\n","# 4. Carregar dataset e modelo pré-treinado\n","transform = transforms.Compose([\n","    transforms.Resize((32, 32)),\n","    transforms.ToTensor(),\n","])\n","val_dataset = CIFAR10(root='.', train=False, download=True, transform=transform)\n","val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n","\n","model_vit = timm.create_model('vit_tiny_patch16_224', pretrained=True)\n","model_vit.head = nn.Linear(model_vit.head.in_features, 10)  # ajuste para CIFAR-10\n","\n","# 5. Avaliação inicial\n","model_vit.eval()\n","correct, total = 0, 0\n","with torch.no_grad():\n","    for images, labels in val_loader:\n","        outputs = model_vit(images)\n","        _, preds = outputs.max(1)\n","        correct += (preds == labels).sum().item()\n","        total += labels.size(0)\n","print(f'Acurácia inicial: {correct/total:.2%}')\n"]}],"metadata":{"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}